{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import emoji\n",
    "\n",
    "# import torchtext.vocab\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = 'train_spam.csv'\n",
    "\n",
    "data_train_all = pd.read_csv(file_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train_all = data_train_all['text']\n",
    "labels_all = data_train_all['text_type']\n",
    "target_all = pd.Categorical(labels_all, categories=['ham', 'spam']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "ENGLISH_STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlink(words):\n",
    "    return  re.sub(r\"http\\S+\", \"\", words)\n",
    "\n",
    "def to_lower(words):\n",
    "    result = words.lower()\n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_underscore(words):\n",
    "    return re.sub(r'_', ' ', words)\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    result = re.sub(r'[!\"#%&\\'()*+,-./:;<=>?@\\[\\]^`{|}~]', ' ', words) # everything except underscore\n",
    "    return result\n",
    "\n",
    "\n",
    "def emoji_to_text(words):\n",
    "    return emoji.demojize(words)\n",
    "\n",
    "def replace_emoji(words,  replace=' <EMOJI> '):\n",
    "    return emoji.replace_emoji(words, replace=replace)\n",
    "\n",
    "def remove_emoji(words):\n",
    "    return replace_emoji(words, '')\n",
    "\n",
    "\n",
    "def replace_non_ascii(words, replace=' <NONASCII> '):\n",
    "    pattern = r'\\b[^\\x00-\\x7F]+\\b'\n",
    "    non_ascii_words = re.sub(pattern, replace, words)\n",
    "    return non_ascii_words\n",
    "\n",
    "\n",
    "def replace_number(words, replace=' <NUMBER> '):\n",
    "    result = re.sub(r'\\b\\w*\\d\\w*\\b', replace, words)\n",
    "    return result\n",
    "\n",
    "def remove_number(words):\n",
    "    return replace_number(words, '')\n",
    "\n",
    "\n",
    "def replace_currency(words, replace=' <CURRENCY> '):\n",
    "    currency_pattern = r'[£$€₹]'\n",
    "    return re.sub(currency_pattern, replace, words)\n",
    "\n",
    "def remove_currency(words):\n",
    "    return replace_currency(words, '')\n",
    "\n",
    "\n",
    "def replace_tag(words, replace=' <TAG> '):\n",
    "    return re.sub(r'\\b\\w*@\\w+\\b', replace, words)\n",
    "\n",
    "def replace_exclamation(words, replace=' <EXCLAMATION> '):\n",
    "    return re.sub(r'!', replace, words)\n",
    "\n",
    "def replace_question(words, replace=' <QUESTION> '):\n",
    "    return re.sub(r'?', replace, words)\n",
    "\n",
    "def replace_slash(words, replace=' <SLASH> '):\n",
    "    return re.sub(r'/', replace, words)\n",
    "\n",
    "def replace_colon(words, replace=' <COLON> '):\n",
    "    return re.sub(r':', replace, words)\n",
    "\n",
    "\n",
    "def remove_whitespace(words):\n",
    "    return words.strip()\n",
    "\n",
    "def replace_newline(words):\n",
    "    return words.replace('\\n', '')\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    return [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "def word_lemmatizer(words):\n",
    "    return [lemmatizer.lemmatize(s) for s in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(sentence, preprocess_utils=None):\n",
    "    if preprocess_utils is None:\n",
    "        preprocess_utils = [\n",
    "                            remove_hyperlink,\n",
    "                            replace_newline,\n",
    "                            to_lower,\n",
    "                            remove_underscore,\n",
    "                            replace_currency,\n",
    "                            replace_number,\n",
    "                            replace_emoji,\n",
    "                            replace_non_ascii,\n",
    "                            remove_punctuation,\n",
    "                            remove_whitespace,\n",
    "                            tokenizer.tokenize,\n",
    "                            remove_stop_words,\n",
    "                            word_lemmatizer,\n",
    "                        ]\n",
    "    for func in preprocess_utils:\n",
    "        sentence = func(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features:\n",
    "- Number of characters in original sentences\n",
    "- Number of words in preprocessed with keyword replacement and tokenized by words\n",
    "- Number of varius keywords (EMOJI, NUMBER, NONASCII, AT, QUESTION, EXCLAMATION, COLON, SLASH, etc)\n",
    "- Number of special words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(texts):\n",
    "    ### character length of text\n",
    "    length_in_charactes = []\n",
    "    for text in texts:\n",
    "        length_in_charactes.append(len(text))\n",
    "    \n",
    "    ### Word length of text\n",
    "    length_in_words = []\n",
    "\n",
    "    preprocess_utils_1 = [\n",
    "        remove_hyperlink,\n",
    "        replace_newline,\n",
    "        to_lower,\n",
    "        remove_underscore,\n",
    "        replace_emoji,\n",
    "        unidecode,       # просто стандартизируем их\n",
    "        remove_punctuation,\n",
    "        remove_whitespace,\n",
    "        tokenizer.tokenize,\n",
    "        remove_stop_words,\n",
    "        word_lemmatizer,\n",
    "    ]\n",
    "    texts_tokenized_1 = [\n",
    "        preprocess_pipeline(sent, preprocess_utils_1) for sent in texts\n",
    "    ]\n",
    "    for text in texts_tokenized_1:\n",
    "        length_in_words.append(len(text))\n",
    "    \n",
    "    ### Preprocess for keywords count\n",
    "    preprocess_utils_2 = [\n",
    "        remove_hyperlink,\n",
    "        replace_newline,\n",
    "        to_lower,\n",
    "        remove_underscore,\n",
    "        replace_currency,\n",
    "        replace_number,\n",
    "        replace_emoji,\n",
    "        replace_non_ascii,\n",
    "        replace_tag,\n",
    "        replace_exclamation,\n",
    "        replace_question,\n",
    "        replace_slash,\n",
    "        replace_colon,\n",
    "        remove_punctuation,\n",
    "        remove_whitespace,\n",
    "        tokenizer.tokenize,\n",
    "        remove_stop_words,\n",
    "        word_lemmatizer,\n",
    "    ]\n",
    "    texts_tokenized_2 = [\n",
    "        preprocess_pipeline(sent, preprocess_utils_2) for sent in texts\n",
    "    ]\n",
    "\n",
    "    n_emojis = []\n",
    "    n_nonascii = []\n",
    "    n_currancy = []\n",
    "    n_number = []\n",
    "    n_tags = []\n",
    "    n_excamation = []\n",
    "    n_question = []\n",
    "    n_slash = []\n",
    "    n_colon = []\n",
    "    for text in texts_tokenized_2:\n",
    "        length_in_words.append(len(text))\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
