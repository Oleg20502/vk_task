{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import emoji\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# import torchtext.vocab\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = 'train_spam.csv'\n",
    "\n",
    "data_train_all = pd.read_csv(file_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train_all = data_train_all['text'].to_list()\n",
    "labels_all = data_train_all['text_type']\n",
    "target_all = pd.Categorical(labels_all, categories=['ham', 'spam']).codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формируем множество важных для классификации слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формируем массивы со спамом и с хамом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число нормальных сообщений: 11469\n",
      "Число спама: 4809\n"
     ]
    }
   ],
   "source": [
    "data_ham = data_train_all[data_train_all['text_type'] == 'ham']\n",
    "data_spam = data_train_all[data_train_all['text_type'] == 'spam']\n",
    "\n",
    "texts_ham = data_ham['text'].to_list()\n",
    "texts_spam = data_spam['text'].to_list()\n",
    "\n",
    "number_of_ham = len(texts_ham)\n",
    "number_of_spam = len(texts_spam)\n",
    "\n",
    "print(f\"Число нормальных сообщений: {number_of_ham}\")\n",
    "print(f\"Число спама: {number_of_spam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции для препроцессинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "ENGLISH_STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(words):\n",
    "    result = words.lower()\n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_underscore(words):\n",
    "    return re.sub(r'_', ' ', words)\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    result = re.sub(r'[!\"#%&\\'()*+,-./:;<=>?@\\[\\]^`{|}~]', ' ', words) # everything except underscore and $\n",
    "    return result\n",
    "\n",
    "\n",
    "def replace_hyperlink(words, replace=' <HTTP> '):\n",
    "    return re.sub(r'http(?:\\S*|\\b)', replace, words)\n",
    "\n",
    "def remove_hyperlink(words):\n",
    "    return replace_hyperlink(words, '')\n",
    "\n",
    "\n",
    "def emoji_to_text(words):\n",
    "    return emoji.demojize(words)\n",
    "\n",
    "def replace_emoji(words,  replace=' <EMOJI> '):\n",
    "    return emoji.replace_emoji(words, replace=replace)\n",
    "\n",
    "def remove_emoji(words):\n",
    "    return replace_emoji(words, '')\n",
    "\n",
    "\n",
    "def replace_non_ascii_words(words, replace=' <NONASCII> '):\n",
    "    pattern = r'\\b[^\\x00-\\x7F]+\\b'\n",
    "    non_ascii_words = re.sub(pattern, replace, words)\n",
    "    return non_ascii_words\n",
    "\n",
    "def replace_non_ascii(words, replace=' <NONASCII> '):\n",
    "    pattern = r'[^\\x00-\\x7F]'\n",
    "    non_ascii_words = re.sub(pattern, replace, words)\n",
    "    return non_ascii_words\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    # return words.encode('ascii', errors='ignore').decode('ascii')\n",
    "    return unidecode(words)\n",
    "\n",
    "\n",
    "def replace_number(words, replace=' <NUMBER> '):\n",
    "    result = re.sub(r'\\w*\\d\\w*', replace, words)\n",
    "    return result\n",
    "\n",
    "def remove_number(words):\n",
    "    return replace_number(words, '')\n",
    "\n",
    "\n",
    "def replace_currency(words, replace=' <CURRENCY> '):\n",
    "    currency_pattern = r'[£$€₹]'\n",
    "    return re.sub(currency_pattern, replace, words)\n",
    "\n",
    "def remove_currency(words):\n",
    "    return replace_currency(words, '')\n",
    "\n",
    "\n",
    "def replace_tag(words, replace=' <TAG> '):\n",
    "    return re.sub(r'\\w*@\\w*', replace, words)\n",
    "\n",
    "def replace_exclamation(words, replace=' <EXCLAMATION> '):\n",
    "    return re.sub(r'!', replace, words)\n",
    "\n",
    "def replace_question(words, replace=' <QUESTION> '):\n",
    "    return re.sub(r'\\?', replace, words)\n",
    "\n",
    "def replace_slash(words, replace=' <SLASH> '):\n",
    "    return re.sub(r'/', replace, words)\n",
    "\n",
    "def replace_colon(words, replace=' <COLON> '):\n",
    "    return re.sub(r':', replace, words)\n",
    "\n",
    "\n",
    "def remove_whitespace(words):\n",
    "    return words.strip()\n",
    "\n",
    "def replace_newline(words):\n",
    "    return words.replace('\\n', ' ')\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    return [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "def word_lemmatizer(words):\n",
    "    return [lemmatizer.lemmatize(s) for s in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(sentence, preprocess_utils=None):\n",
    "    if preprocess_utils is None:\n",
    "        preprocess_utils = [\n",
    "            to_lower,\n",
    "            replace_newline,\n",
    "            remove_underscore,\n",
    "            replace_hyperlink,\n",
    "            replace_emoji,\n",
    "            replace_non_ascii_words,\n",
    "            replace_non_ascii,\n",
    "            remove_non_ascii,\n",
    "            remove_punctuation,\n",
    "            remove_whitespace,\n",
    "            tokenizer.tokenize,\n",
    "            remove_stop_words,\n",
    "            word_lemmatizer,\n",
    "        ]\n",
    "    for func in preprocess_utils:\n",
    "        sentence = func(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_utils = [\n",
    "    to_lower,\n",
    "    replace_newline,\n",
    "    replace_hyperlink,\n",
    "    remove_underscore,\n",
    "    replace_currency,\n",
    "    replace_emoji,\n",
    "    replace_non_ascii_words,\n",
    "    replace_non_ascii,\n",
    "    remove_non_ascii,\n",
    "    replace_number,\n",
    "    replace_tag,\n",
    "    replace_exclamation,\n",
    "    replace_question,\n",
    "    replace_slash,\n",
    "    replace_colon,\n",
    "    remove_punctuation,\n",
    "    remove_whitespace,\n",
    "    tokenizer.tokenize,\n",
    "    remove_stop_words,\n",
    "    word_lemmatizer,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем ham и spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ham_tokenized = [\n",
    "    preprocess_pipeline(sent, preprocess_utils) for sent in texts_ham\n",
    "]\n",
    "\n",
    "texts_spam_tokenized = [\n",
    "    preprocess_pipeline(sent, preprocess_utils) for sent in texts_spam\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим словари и выделяем топ важных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of ham vocab 28363\n",
      "Len of spam vocab 17229\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(tokenized_sentences):\n",
    "    counter = Counter()\n",
    "    for sentence in tokenized_sentences:\n",
    "        counter.update(sentence)\n",
    "    return counter\n",
    "\n",
    "ham_vocab = build_vocab(texts_ham_tokenized)\n",
    "print(f'Len of ham vocab {len(ham_vocab)}')\n",
    "\n",
    "spam_vocab = build_vocab(texts_spam_tokenized)\n",
    "print(f'Len of spam vocab {len(spam_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ham important words 63\n",
      "Number of spam important words 74\n"
     ]
    }
   ],
   "source": [
    "def get_important_words(vocab, texts_number, threshold = 0.05):\n",
    "    top_words = vocab.most_common()\n",
    "    important_words = set()\n",
    "    for word, count in top_words:\n",
    "        w = count / texts_number\n",
    "        if w >= threshold:\n",
    "            important_words.add(word)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return important_words\n",
    "\n",
    "ham_important_words = get_important_words(ham_vocab, number_of_ham)\n",
    "spam_important_words = get_important_words(spam_vocab, number_of_spam)\n",
    "\n",
    "print('Number of ham important words', len(ham_important_words))\n",
    "print('Number of spam important words', len(spam_important_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all important words 110\n"
     ]
    }
   ],
   "source": [
    "important_words = ham_important_words.union(spam_important_words)\n",
    "print('Number of all important words', len(important_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features:\n",
    "- Number of characters in original sentences\n",
    "- Number of words in preprocessed with keyword replacement and tokenized by words\n",
    "- Number of varius keywords (EMOJI, NUMBER, NONASCII, AT, QUESTION, EXCLAMATION, COLON, SLASH, etc)\n",
    "- Number of other important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(texts, preprocess_utils, important_words):\n",
    "    N_texts = len(texts)\n",
    "\n",
    "    ### character length of text\n",
    "    length_in_charactes = np.zeros((N_texts, 1))\n",
    "    for i, text in enumerate(texts):\n",
    "        length_in_charactes[i] = len(text)\n",
    "    \n",
    "\n",
    "    ### Word length of text\n",
    "    texts_tokenized = [\n",
    "        preprocess_pipeline(sent, preprocess_utils) for sent in texts\n",
    "    ]\n",
    "\n",
    "    length_in_words = np.zeros((N_texts, 1))\n",
    "\n",
    "    for i, text in enumerate(texts_tokenized):\n",
    "        length_in_words[i] = len(text)    # some can be zero!\n",
    "    \n",
    "\n",
    "    ### Important words count\n",
    "\n",
    "    n_important_words = np.zeros((N_texts, len(important_words)))\n",
    "    for i, text in enumerate(texts_tokenized):\n",
    "        text_joined = ' '.join(text)\n",
    "        for j, word in enumerate(important_words):\n",
    "            n_important_words[i, j] = text_joined.count(word) / (length_in_words[i] + 0.00001)\n",
    "    \n",
    "    X = np.hstack([length_in_charactes, length_in_words, n_important_words])\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = featurize(texts_train_all, preprocess_utils, important_words)\n",
    "y = target_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "                                    X, y,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=42, \n",
    "                                    stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы несбалансированы, но для оценки качества используется ROC AUC, который нечуствителен к дисбалансу. Поэтому каких-то дополнительных действий для борьбы с влиянием дисбаланса при подготовке данных и при обучении моделей делать не буду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(model, X_test, y_test):\n",
    "    prob_pred = model.predict_proba(X_test)\n",
    "    y_pred = np.argmax(prob_pred, axis=1)\n",
    "    p_pred = prob_pred[:, 1]\n",
    "\n",
    "    return (\n",
    "        accuracy_score(y_test, y_pred),\n",
    "        precision_score(y_test, y_pred),\n",
    "        recall_score(y_test, y_pred),\n",
    "        f1_score(y_test, y_pred),\n",
    "        roc_auc_score(y_test, p_pred)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1000.0}\n",
      "Accuracy 0.8673, Precision 0.8111, Recall 0.7180, f1 0.7617, ROC AUC 0.94010\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'C': np.logspace(0, 3, 6)\n",
    "}\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, solver='saga', random_state=42)\n",
    "\n",
    "grid = GridSearchCV(model, params, scoring='roc_auc', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "(accuracy_val,\n",
    "precision_val,\n",
    "recall_val,\n",
    "f1_val,\n",
    "roc_auc_val) = calc_metrics(grid.best_estimator_, X_val, y_val)\n",
    "\n",
    "print(f'Accuracy {accuracy_val:.4f}, Precision {precision_val:.4f}, ', end='')\n",
    "print(f'Recall {recall_val:.4f}, f1 {f1_val:.4f}, ROC AUC {roc_auc_val:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10.0}\n",
      "Accuracy 0.9290, Precision 0.9087, Recall 0.8444, f1 0.8754, ROC AUC 0.97158\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'C': np.logspace(-1, 1, 3)\n",
    "}\n",
    "\n",
    "model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(model, params, scoring='roc_auc', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "(accuracy_val,\n",
    "precision_val,\n",
    "recall_val,\n",
    "f1_val,\n",
    "roc_auc_val) = calc_metrics(grid.best_estimator_, X_val, y_val)\n",
    "\n",
    "print(f'Accuracy {accuracy_val:.4f}, Precision {precision_val:.4f}, ', end='')\n",
    "print(f'Recall {recall_val:.4f}, f1 {f1_val:.4f}, ROC AUC {roc_auc_val:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "Accuracy 0.9292, Precision 0.9561, Recall 0.7970, f1 0.8693, ROC AUC 0.97434\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [50, 100, 500],\n",
    "    'max_depth': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 3, 5, 10],\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid = GridSearchCV(model, params, scoring='roc_auc', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "(accuracy_val,\n",
    "precision_val,\n",
    "recall_val,\n",
    "f1_val,\n",
    "roc_auc_val) = calc_metrics(grid.best_estimator_, X_val, y_val)\n",
    "\n",
    "print(f'Accuracy {accuracy_val:.4f}, Precision {precision_val:.4f}, ', end='')\n",
    "print(f'Recall {recall_val:.4f}, f1 {f1_val:.4f}, ROC AUC {roc_auc_val:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.03162277660168379, 'max_depth': 10}\n",
      "Accuracy 0.9408, Precision 0.9271, Recall 0.8677, f1 0.8964, ROC AUC 0.97969\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth': [1, 2, 3, 4, 5, 6, 7, 10, 15, 20],\n",
    "    'learning_rate': np.logspace(-2, 0, 5)\n",
    "}\n",
    "\n",
    "model = XGBClassifier(n_estimators=500, seed=42, n_jobs=-1)\n",
    "\n",
    "grid = GridSearchCV(model, params, scoring='roc_auc', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "(accuracy_val,\n",
    "precision_val,\n",
    "recall_val,\n",
    "f1_val,\n",
    "roc_auc_val) = calc_metrics(grid.best_estimator_, X_val, y_val)\n",
    "\n",
    "print(f'Accuracy {accuracy_val:.4f}, Precision {precision_val:.4f}, ', end='')\n",
    "print(f'Recall {recall_val:.4f}, f1 {f1_val:.4f}, ROC AUC {roc_auc_val:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7071, Precision 0.5023, Recall 0.9251, f1 0.6511, ROC AUC 0.86016\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "(accuracy_val,\n",
    "precision_val,\n",
    "recall_val,\n",
    "f1_val,\n",
    "roc_auc_val) = calc_metrics(model, X_val, y_val)\n",
    "\n",
    "print(f'Accuracy {accuracy_val:.4f}, Precision {precision_val:.4f}, ', end='')\n",
    "print(f'Recall {recall_val:.4f}, f1 {f1_val:.4f}, ROC AUC {roc_auc_val:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### На данных фичах лучшим оказался градиентный бустинг на деревьях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
